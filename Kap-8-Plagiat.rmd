---
title: "Glanz und Misere des Plagiats. Zur Attraktivierung der Medien"
indent: true
output:
    html_document: default
bibliography: Rel-Bib.bib
lang: de-De
header-includes:
  - \usepackage[german=quotes]{csquotes}
---

Früher in der Öffentlichkeit wenig thematisiert, ist es jetzt beinah unmöglich, den Ausdruck _KI_ für künstliche Intelligenz zu verfehlen. Es ist die neue digitale Mediation _en vogue_, darum sich viele gesellschaftliche Debatten und Kontroversen drehen, seitdem diese Technologie unter dem Namen _ChatGPT_ -- GPT für _Generative Pretrained Transformers_ -- bekannt und im Internet zur Verfügung gestellt wurde. Wer _ChatGPT_ probiert hat, hat ein Gefühl dafür gewonnen, in welche Richtung die nächsten Entwicklungen im Internet und in den sozialen Medien gehen und welche möglicherweise weitreichende Veränderungen unsere Gesellschaften durchgehen würden, die heutzutage nicht mehr als von digitalen Relationen in der Form von zahlreichen Algorithmen vernetzten Gesellschaften weg zu denken sind [@Seyfert2019; @Seyfert2021]. _ChatGPT_ zeigt allen Akteuren und Instanzen unserer Gesellschaften vielleicht deutlicher als andere Mediationen zuvor, dass die Macht von Medien und im Allgemeinen von Mediationen deshalb so bewältigend ist, weil sie unsere Verhältnisse zu den anderen und zur Welt abfangen und reproduzieren können bzw. weil sie jede Stufe der Arbeit an relationalen Ereignissen entwickeln können, ohne dass dabei Akteure und gesellschaftliche Instanzen notwendig wären. Die Welt der Mediationen ist eine Welt, in der idealerweise nicht mehr die Akteure und die Kollektiven anhand von Verhältnissen die Mediationen dieser Verhältnisse kontrollieren, sondern diese Mediationen kontrollieren diese Verhältnisse anhand von Akteuren und Kollektiven. Deshalb bilden die Mediationen die Grundlage von einer eigenen Ordnung von Verhältnissen, die wie "an sich" existieren würden bzw. in der die Relation vollständig frei von menschlichen Merkmalen und in diesem Sinne als reine perfekte Relation erfolgen würde. Diese Ordnung von Verhältnissen fungiert als Grundlage bzw. Selbstverständnis der Medien orientierten Relationsstruktur. In dieser Relationsstruktur sollen die Mediationen die doppelte Einschreibung von Akteuren und Instanzen gewährleisten, diese Akteure auf Zirkulation bringen und die Position von Instanzen dieser Relationsstruktur stärken. Entsprechend arbeiten die Akteure und die Instanzen in dieser Relationsstruktur daran, die Mediationen möglichst unabhängig von der Kontrolle der Akteure und der gesellschaftlichen Instanzen zu machen, damit diese Mediationen ihre eigene Kontrolle in der Gewährleistung der doppelten Einschreibung von Akteuren und Instanzen entwickeln, die sich als die Bedeutung der Reziprozität und die Grundlage der Legitimation von Akteuren und Instanzen in dieser Relationsstruktur ergibt. Gleichzeitig ist diese Arbeit der Beleg für die Abweichung der Medien orientierten Relationsstruktur vom Ideal der perfekten Relation, was wiederum diese Relationsstruktur strukturiert und im Vergleich zu den anderen Relationsstrukturen ihre Identität als Ordnung von Verhältnissen gibt, die zur Attraktivität der Mediationen beitragen müssen. Diese Attraktivität ist das Hauptmerkmal und die Kernsequenz dieser Relationsstruktur. Sie ist der Punkt, ab der sie sich als selbstständige Relationsstruktur bildet.

Wie ist es dazu gekommen bzw. wie hat sich aus der Verselbständigung von Mediationen eine ganze Relationsstruktur gebildet? In diesem Kapitel beantworten wir diese Frage zuerst anhand von einem vertiefenden Blick in die Geschichte der Operationen, die ein Transformer durchführt. Dabei erklären wir einerseits, wie diese Operationen jede Stufe der Arbeit an relationalen Ereignissen abfangen, die zum einen auf Mediationen transferiert werden und zum zweiten an diesen Mediationen miteinander verbunden werden. Diese zwei Schritte sollen zur Attraktivität dieser Mediation als Repräsentanten nicht nur von anderen Mediationen, sondern und wie oben hervorgehoben auch und vor allem von der sozialen Arbeit an relationalen Ereignissen beitragen. Aus der erfolgreichen Entwicklung von diesen Schritten ergibt sich, wie und wie weit diese Mediationen in Relationsstrukturen durchgesetzt werden können. In dieser Hinsicht und in Verbindung mit der KI als Transformer kann deshalb gefragt werden, wie sie sich tatsächlich durchsetzen kann und welche Folgen ihre mögliche Durchsetzung hat. Um diesem letzten Punkt besser veranschaulichen zu können, machen wir abschließend ein Parallel mit einer anderen Mediation, die sich erfolgreich durch alle Relationsstrukturen verbreitet hat: Das Geld.

## Transformers als relationale Dispositiven

Die Theorie der Relation versteht Mediationen als Träger von Verhältnissen, davon die jeweiligen Mediationen ein Prinzip des Arrangements dieser Verhältnisse darstellen. Deshalb versteht sie Mediationen nicht nur im Sinne von Trägern von Funktionen oder von Formen, sondern auch als relationale Dispositiven. Damit solche relationale Dispositiven erfolgreich entwickelt werden, ist es jedoch erforderlich, dass Mediationen zu abstrakteren Mediationen entwickelt werden. Nur so können unterschiedliche Verhältnisse bzw. Verhältnisse, die auf unterschiedliche Kontexte verweisen, miteinander verbunden werden, ohne dass die jeweiligen Kontexte, die als Organisationsrahmen und Sinnzusammenhang der entsprechenden Verhältnisse gelten, diese Verbindungen schwächen oder stören. Diese Abflachung von Kontexteffekten macht aus Mediationen mächtige Akkumulatoren und Generatoren von Verhältnissen, die überall in Relationsstrukturen und Sequenzen von Relationsstrukturen verbreiten werden können, was der Art und Weise entspricht, wie die Medien orienterten Relationsstruktur die anderen Relationsstruktur zum Vorteil der eigenen Verbreitung und der Verbreitung der eigenen Fassung der Reziprozität als medialer Kontrolle satellisiert. Ein solches Verfahren ist dasjenige, dass in der Forschung zu Sprachen und zur Modellierung von Sprachen seit den 1950er Jahren entwickelt wurde, davon die Transformers wie _Generative Pretrained Transformers_ das zeitgenössische Ergebnis sind [@Hutchins1995].

Sprachmodelle wurden entwickelt, um das spezifische Problem der Übersetzung von einer Sprache in eine andere Sprache mit Hilfe eines Rechners zu lösen (ebd.). Bis in die 1980er Jahren erweisen sich die meist mit Unterstützung von der Firma _IBM_ durchgeführten Übersetzungsexperimente jedoch als bescheiden. Mit einem Rechner war es möglich, Begriffe in einer begrenzten Anzahl von Texten zu übersetzen, die anhand von einem statistischen Verfahren in Sätzen aufgegriffen und übersetzt wurden. Jedoch konnte ein solches Verfahren keine langen Wortfolgen wie etwa lange Sätze verarbeiten. Es gaben häufig Fehler, die ein menschliches Eingreifen zur Überprüfung und Korrektur der Übersetzung erforderten [@Pierce1966; @Zong2022]. Der technische Fortschritt, insbesondere die Steigerung der Rechenleistung (bessere _Central Processing Unit_ und _Graphics Processing Unit_) und des Arbeitsspeichers hat dazu geführt, dass einerseits größere Datenmengen gespeichert werden konnten und andererseits den Umfang von statistischen Operationen über diese Daten erweitert werden konnten. Aus dieser Entwicklung ergibt sich Mitte der 1980er Jahre das _Recurrent Neural Network_ (RNN). Es beschreibt eine Methode, die Verhältnisse zwischen Begriffen in Sprachen auf der Grundlage der Modellierung von Verhältnissen zwischen Neuronen im menschlichen Gehirn auffasst. Mit der RNN-Methode können mehr Korrespondenzen zwischen Begriffen hergestellt -- das sog. _deep learning_ Ansatz  -- und gleichzeitig mit größeren Wortzusammenstellungen gearbeitet werden. Daraus sind ausgefeilteren Sprachmodellen wie etwa das Modell der bidirektionalen rekurrenten neuronalen Netzwerken (BRNN), das Langzeitgedächtnismodell (LSTM) oder das _gated recurrent units_ (GRUs) Modell konstruiert worden [siehe @Hochreiter1997]. Solche Modelle sind Matrixen von Verbindungen zwischen Begriffen, die nach einem Ähnlichkeitsansatz quantitativ gewertet werden. Begriffe, die stark verbunden sind, gelten als Begriffe, die auf ähnliche Inhalte verweisen. Begriffe, die dagegen schwach verbunden sind, verweisen auf unterschiedliche Inhalte. Ein solches Sprachmodell kann dann als eine Referenzmatrix verwendet werden, um weiter Sammlungen von sprachlichen Daten zu untersuchen und deren Inhalt zu werten.

Selbst mit der weiteren Entwicklung von Rechnern bleibt es jedoch schwierig, solche Sprachmodelle auf große Datensammlungen anzuwenden. Oft reichen die rechnerischen Kapazitäten oder die Speicherkapazitäten von gewöhnlichen Rechnern nicht aus. Anfang 2017 wurden deshalb GPT-Modelle entwickelt, die dieses Problem lösen sollten. Das innovative Merkmal von GPT-Modellen besteht darin, Matrixen von Begriffen nicht mit Sequenzen von Begriffen -- etwa mit Sätzen -- zu verbinden, sondern mit einzelnen Begriffen. Ein Transformer verbindet also ein Begriff mit den Begriffen, die am wahrscheinlichsten mit ihm in einer bestimmten Sprache auf der Grundlage von einer bestimmten Datensammlung verbunden sind. GPT-Modelle sind deshalb einerseits eine Erweiterung der Sprachmodelle, die nach einem _deep learning_ Ansatz konstruiert wurden. Andererseits besteht ihre Besonderheit darin, dass sie aufgrund von solche Matrixen die Wahrscheinlichkeit schneller und mit wenigen rechnerischen Kosten bestimmen können, dass ein Begriff _B_ nach einem Begriff _A_ auf der Grundlage von einem Kontext von Begriffen auftaucht, die am wahrscheinlichsten mit dem Begriff _A_ in einer gegebenen Sprache verbunden sind [siehe auch @Devlin2019]. In künftigen Versionen von solchen Transformern handelt es sich darum, das Ergebnis dieser Wahrscheinlichkeitsrechnung zu benutzen, um die Effizienz des Sprachmodells selbst bzw. um die Genauigkeit der Matrixen zu erhöhen, damit solche Wahrscheinlichkeitsrechnungen schneller und präziser erfolgen, ohne dass die rechnerischen Ressourcen zu sehr steigen. Bei Transformers geht es also um die Akkumulation von Dateien und von Verbindungen zwischen diesen Dateien, die erlauben, weitere Verbindungen zwischen diesen Dateien und entsprechend weitere Dateien aus diesen Verbindungen zu generieren. Solche Akkumulation anhand und in der Form von Mediationen zu entwickeln, sind jedoch nicht erst mit der wissenschaftlichen Arbeit an der KI entstanden. Sie stellen eine Grundtendenz beim Einbezug von Mediationen durch Akteure und gesellschaftliche Instanzen dar. Mediationen eröffnen Möglichkeiten zum weiteren Abgreifen von Verhältnissen, was die Akkumulationsmacht von Mediationen stärkt und zur Stärkung ihrer Symbolisierungsmacht und Formalisierungsmacht beiträgt. Dies macht Mediationen für Akteure und gesellschaftliche Instanzen attraktiv, auch wenn dabei sich diese Mediationen der Kontrolle von Akteuren und Instanzen immer mehr entziehen. Nehmen wir ein Beispiel, dass mit einer Grundlageoperation von Transformers verbunden ist: Lesen.

## Lesen

Transformers lesen Dateien und dabei machen sie darauf aufmerksam, wie die Praxis des Lesens in der Zeit verändert wurde, in dem das Lesen immer weiter vom Leser externalisiert wird, was zu weiteren Operationen führt, die auf der Grundlage dessen durchgeführt werden, was gelesen wurde.









Mit dieser einführende Beschreibung von Sprachmodellen sehen wir bereits, dass auf der Ebene der Transformers als Mediationen Operationen stattfinden, die nicht in ihrer Bedeutung sondern in ihrer Verbindung neu sind.




Dadurch können die GPT-Modelle potenziell auf beliebig große Datensammlungen angewendet werden, ohne dass sich die Qualität der Vorhersagen entsprechend verändert zu schnell.


Im Vergleich zu bereits existierenden Sprachmodellen sind die neueren GPT-Modelle wie GPT-3 Allzweckmodelle. Sie können ein sehr breites Spektrum von Problemen lösen [von einfachen Frage-Antwort-Problemen des Allgemeinwissens, über Internet-Suchmaschinen, Chatbots zur Unterstützung von Kundenzentren und Hotlines, bis hin zu spezialisierten Fragen des wissenschaftlichen Wissens, wie derzeit in der Medizin; siehe @Zong2022], was schließlich ihre Implementierung in verschiedenen gesellschaftlichen Tätigkeitsbereichen unterstützt. In dieser Hinsicht unterscheiden sie sich von spezifischeren Sprachmodellen, die zur Klassifizierung von Dokumenten oder zur Rekonstruktion von Themen oder semantischen Wortketten, die diese Dokumente strukturieren, verwendet werden, wie z.B. die Sprachmodelle, die im Rahmen klassischer NLP-Aufgaben wie Named Entity Recognition (NER), Sentimentanalyse oder Themenmodellierung eingesetzt werden [für einen Überblick siehe @PapilloudHinneburg2018]. Erste öffentliche Experimente mit GPT-Modellen fanden schnell Beachtung in Tageszeitungen und damit verbundenen Berichten, vor allem über die Beta-Version eines GPT-3-Modells, das OpenAI im Herbst 2021 im Internet veröffentlichte (vgl. https://www.openai.com). OpenAI ist ein Data-Science-Unternehmen, das sich auf die Entwicklung und den Betrieb von Algorithmen oder intelligenten Robotern spezialisiert hat und einen Chatbot-Prototyp namens _ChatGPT_ veröffentlicht hat. ChatGPT verwendet ein GPT-Sprachmodell, das auf der Grundlage von zwei Reinforcement-Learning-Verfahren trainiert wurde. Das Modell verhält sich wie eine (quasi-)selbstdynamische und (quasi-)selbsttätige Software, die schrittweise Beziehungen zwischen Textelementen erlernt, um sowohl positive als auch negative Vorhersagen über zukünftige Beziehungen zwischen diesen Elementen zu maximieren (vgl. \url{https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/} \url{https://arxiv.org/abs/1706.03741}). Es wurde auf fünf Milliarden hauptsächlich textueller Daten aus dem Internet trainiert, darunter Bücher, Wikipedia, Konversationen in sozialen Medien, wissenschaftliche Veröffentlichungen (\url{https://www.scientificamerican.com/article/we-asked-gpt-3-to-write-an-academic-paper-about-itself-mdash-then-we-tried-to-get-it-published/}) und Websites. Es ist in der Lage, lange Textsequenzen zu verarbeiten und daraus selbstständig menschenähnliche Texte als Antwort auf eine natürlichsprachliche Eingabe (Fragen, Aussagen oder Befehle) an einer Befehlszeile zu generieren (vgl. \url{https://dev.to/abbhiishek/chatgpt-the-ultimate-tool-for-natural-language-processing-and-text-generation-40ag}). Die generierten Texte können unterschiedlicher Natur sein, von Kochrezepten bis hin zu hochkomplexen Artikeln, die praktisch wissenschaftlichen Standards genügen (url{https://www.scientificamerican.com/article/we-asked-gpt-3-to-write-an-academic-paper-about-itself-mdash-then-we-tried-to-get-it-published/}).


In der wissenschaftlichen Gemeinschaft und neben der überwiegenden Mehrheit der Fachliteratur, die auf die Bereitstellung verbesserter Sprachmodelle abzielt, haben GPT-Modelle eine Debatte über den Wert von Wissen im Zusammenspiel zwischen Menschen und digitalen Technologien angeregt, die teilweise auf die Debatten über verteilte Kognition in den Neuro- und Kognitionswissenschaften zurückgeht [@Magnus2007; @Michaelian2013; @Anderson2018; @Risku2020], sowie aus den Debatten über die Rolle von Algorithmen zur Unterstützung menschlicher Entscheidungen [@Pariser2012; @Anderson2012; @Braverman2014; @Bucher2018; @LeeLarsen2019]. Laut Zhang et al. (2021) haben transformatorbasierte Sprachmodelle menschenähnliche Sprachverstehensfähigkeiten erreicht und bieten eine intuitive Methode der natürlichsprachbasierten Benutzerinteraktion. In dem Maße, wie KI mehr Verantwortung übernimmt, wird sich die Rolle des Menschen in Wissensprozessen verändern. Man könnte annehmen, dass breit angelegte neuronale Sprachmodelle auf der Grundlage von Transformatoren mit Milliarden von Parametern für die Datenbereinigung, Vektorisierung und Berechnung der Einbettung von Wörtern, die es ermöglichen, Wissen zu zerlegen und in völlig neuen Kontexten wiederzuverwenden, den Menschen dazu ermutigen würden, bestehende mentale Modelle aufzubrechen und seine Wissensprozesse auf andere Weise zu fokussieren. Solche Sprachmodelle sind jedoch nicht frei von verschiedenen Schwierigkeiten.

Da sie nach wie vor hauptsächlich auf einseitigen Kodierungs-Dekodierungs-Transformatoren und statistischen Wahrscheinlichkeiten zur Vorhersage der folgenden Token beruhen, scheitern GPT-Modelle tendenziell bei einfachen reversiblen Antworten, die mit allgemeinem Wissen gelöst werden können. Sie neigen auch dazu, Klassifizierungsaufgaben in spezifischen Korpussen, die sich auf spezifischere Themen beziehen und weniger, d.h. hunderte bis tausende von Dokumenten enthalten, nicht korrekt zu lösen [@Floridi2020]. GPT-basierte Sprachmodelle beinhalten keine Regulierungspolitik, die den Missbrauch solcher Modelle verhindern würde [@Dehouche2021; @Brown2020]. Sie erzeugen auch Verzerrungen in Bezug auf die Darstellung der Geschlechter, indem sie Frauen zugunsten von Männern diskriminieren [@Lucy2021; @Shihadeh2022]. Sie erzeugen rassische und religiöse Vorurteile, indem sie Menschen aufgrund ihrer Hautfarbe oder ihres Glaubens diskriminieren und weiße Christen bevorzugen, wie z. B. bei der Verwendung von GPT-Modellen in der Medizin im Zusammenhang mit Schmerzmanagement und Schmerzbehandlung [@Loge2021]. Darüber hinaus werden Sprachmodelle ständig weiterentwickelt, mit dem Ziel, die aktuellen GPT-Modelle zu übertreffen, indem die Modellgröße erhöht wird, bis eine andere Architektur vorgeschlagen wird, um die Transformatoren zu ersetzen, wobei solche Modelle eine beträchtliche Menge an Ressourcen verbrauchen, was zu ernsthaften Umweltproblemen führt [@Zong2022].




Bei KI wie _ChatGPT_ geht es nicht nur darum, intelligente digitalen Suchmachinen zu konstruieren, die Suchergebnisse auf der Grundlage von um Suchbegriffe semantisch besten aggregierten Begriffsketten bestimmen, oder Anwendungen zu programmieren, die etwa aus ausgewählten Bildern Kunstwerke oder aus ausgewählten Sammlungen von Informationen wissenschaftliche Aufsätze, Expertenberichte, medizinische Diagnosen produzieren können. Es geht auch und vielleicht insbesondere darum, optimale Kontexte herauszuarbeiten, die als relationale Dispositiven entwickelt werden, um Einschreibungsakte von Akteuren und Instanzen abgesehen von derer Besonderheit zu gewährleisten. In diesem Sinne ist die _KI_ in der Form von _ChatGPT_ nichts anders als die nächste Entwicklung aus Satellisierungsstrategien von der Relationsstruktur, die im Laufe der Verselbständigung von Mediationen gegenüber Akteuren und deren Zirkulation sowie Instanzen und deren Position gebildet wurde, d.h. die Medien orientierte Relationsstruktur.



_Kontext_: Technologien wie _ChatGPT_ sind _Generative Pretrained Transformers_ (GPT), die auf der Grundlage von Datenbanken mit unterschiedlichen Informationen in Textform, Bildern, Tönen, Gesprächen usw. mit dem Ziel entwickelt werden, Verbindungen zwischen Inhalten dieser Informationsquellen je nachdem herzustellen, wie oft bestimmte Merkmale (Begriffe, Farben, Formen, Klänge usw.) in einem ausgewählten Umfang von diesen Quellen zusammen auftauchen. Daraus ergibt sich ein Kontext von Verbindungen, die als _relationale Dispositiven_ entwickelt werden bzw. die als abstrakte Kontexte der Integration von weiteren und bevorzugt abstrakten Mediationen zur Entwicklung von Formalisierungsoperationen auf jeder Ebenen einer Relationsstruktur mit dem Ziel fungieren, Einschreibungsakte überall und konstant gewährleisten zu können. Relationale Dispositiven sind umso stärker, als sie durch das abstrakte Merkmal der Mediationen, die sie enthalten, jede Art von relationalen Ereignissen abfangen, in Verbindung mit anderen relationalen Ereignissen setzen, solche Verbindung werten und sie auf Einbezug in Zirkulationen von Akteuren vorbereiten können.

Transformers wie GPT enthalten tausende Parameters, die soviele Mediationen von Transformers sind, die beeinflussen die Art und Weise, wie welcher Inhaltsteil ausgepickt und wie er mit anderen Inhaltsteile verbunden wird, wie eine solche Verbindung mit Verbindungen zwischen anderen Inhaltsteilen gewertet wird, und wie dabei Gruppen von Verbindungen voneinander nach derer Bedeutung im Rahmen von einem bestimmten Kontext erkannt, verglichen, diskriminiert bzw. nach Wichtigkeit hierarchisiert werden können.

Entsprechend breiter und vielfältiger werden die Einsatzgebiete von solchen relationalen Dispositiven, weil die Mediationen, die sie ausmachen, nicht an bestimmten Zirkulationen, Sequenzen oder Relationsstrukturen gebunden sind, sondern dazu beitragen, dass relationale Dispositiven überall quer durch Sequenzen und Relationsstrukturen ausgebreitet werden können. Die Verwendung von Transformers auf Sammlungen von Dateien findet heute Anwendungen in allen einzelnen Tätigkeitsbereichen der Gesellschaft. Dies macht diese relationalen Dispositiven in ihrer Entwicklung und Verbreitung nicht nur von Zirkulationen, Sequenzen oder Relationsstrukturen unabhängig. Dies gibt ihnen ebenfalls eine relative Unabhängigkeitsmacht gegenüber Akteuren und Instanzen in Relationsstrukturen, weil sie einerseits Einschreibungsakte virtuell überall in Sequenzen und Relationsstrukturen ermöglichen, die die Einschreibung von Akteure und Instanzen in solchen Sequenzen und Relationsstrukturen weniger Zeit und Aufwand kostet, was andererseits jedoch voraussetzt, dass diese Akteure und Instanzen solche Einschreibungsakte weniger gut kontrollieren können. Die Kontrolle gehört dem relationalen Dispositiv als die konkrete Wirkung der Zusammenstellung von unterschiedlichen Mediationen an, die auf die Durchsetzung vom formalen Verfahren abzielt, das ein relationaler Dispositiv verkörpert und davon er die Bedeutung ausmacht. Im Fall von Transformers geht es um das Abgreifen der besten oder best möglichen Verbindungen zwischen Inhaltsteilen im Bezug auf eine Dateiensammlung, daraus Formalisierungsverfahren gestaltet werden -- einen Bericht schreiben, eine Statistik herstellen, eine Anwendung programmieren, ein Musikstück verfassen, ein Kunstwerk anfertigen usw. --, die einer Einschreibungsakte dienen. Was die beste oder best mögliche Verbindung bedeutet, ergibt sich vom Transformer, der in vielen Iterationsverfahren auf der Dateiensammlung all mögliche Verbindungen zwischen Inhaltsteilen probiert und prüft, bevor er sie von gut bis schlecht hierarchisiert. Diese Kontrolle ist die Auffassung der Reziprozität und die Grundlage der Legitimation von Akteure und Instanzen, die jede Art von Mediation in sich trägt, und die ihre Bedeutung als Alleinstellungs- und Hauptmerkmal der Medien orientierten Relationsstruktur findet. Durch die Verbreitung von relationalen Dispositiven und ihre Implementierung in den anderen Relationsstruktur verbreitet sich diese spezielle Bedeutung der Reziprozität auf die anderen Relationsstrukturen. Sie verdeutlich die Art und Weise, wie die Medien orientierte Relationsstruktur die drei anderen Relationsstrukturen satellisiert bzw. unter ihrer Kontrolle zu bringen versucht. Dabei macht sie gleichzeitig deutlich, das ihr Ideal der perfekten Relation das perfekte relationale Dispositiv ist, das in der Lage wäre, selbständige Einschreibungsakte als Ergebnis des eigenen Formalisierungsverfahrens überall in Relationsstrukturen jenseits der besonderen Merkmalen von Akteuren und von ihrer Zirkulation sowie von Instanzen und von ihrer Position in Sequenzen von solchen Relationsstrukturen. Nicht Akteure und Instanzen tätigen hier primär Einschreibungsakten, sondern Mediationen, die auf alle Akteure und Instanzen offen sind, und ihnen Kontrollemöglichkeiten anbieten, die zu Legitimationsmöglichkeiten entwickelt werden. In der Medien orientierten Relationsstruktur gipfelt dieses Selbstverständnis der Relationsstruktur in der Kernsequenz der Attraktivität als Attraktivität von Mediationen, die solche relationale Dispositiven unterstützen müssen, damit die Zirkulation in dieser Relationsstruktur gestärkt wird.

Die Grundeigenschaften von Transformers sind komplex, wenn man sich mit den unterschiedlichen Mediationen bzw. Parametern beschäftigt, die sie enthalten. Wenn man dagegen die Operationen berücksichtigt, die solche Mediationen durch ihre Parametern durchführen, zeigen Transformers grundsätzliche Ähnlichkeiten mit Mediationen, die zu den ersten relationalen Dispositiven entwickelt wurden, wie etwa Zitatspraktiken oder die Praxis des Exzerpierens. In dieser Hinsicht sind Transformers wie Zitatspraktiken und Praktiken des Exerpierens konkrete Mittel zur Investition in die Repräsentation der Attraktivität von Mediationen und deren funktionalen, formalen und relationalen Merkmalen.







Es geht grundsätzlicher um die Hierarchisierung von Mediationen als Prinzip der Durchsetzung derer Attraktivität zur Unterstützung der Verbreitung der medien-orientierten Relationsstruktur.

Diese Antwort entwickeln wir in diesem Kapitel am besonderen Beispiel des Plagiats, das mit der breiteren Frage der Legitimität von Wissen, von Akteuren, von Instanzen und von Mediationen des Wissens verbunden ist. Wenn wir das Plagiat durch Relation in Zirkulation denken, dann wird es deutlich, dass wir hier mit einer Praxis zu tun haben, die sehr lange Wurzel in der Geschichte hat und mit unterschiedlichen Formen des Umgangs mit Wissen, Wissensbeständen und Wissenstraditionen verbunden ist -- etwa das Kopieren, das Exzerpieren, das Kompilieren vom Wissen und von Wissensbeständen, die jede Form der Übertragung oder der Überlieferung vom Wissen und von Wissensbeständen zugrunde liegt.
